{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data set\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"d4rk3r/resumes-raw-pdf\")\n",
    "dataset.features\n",
    "pdf_item = dataset[0]\n",
    "print(f\"PDF type: {type(pdf_item['pdf'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading web dataset from url\n",
    "\n",
    "import webdataset as wds\n",
    "url = \"https://huggingface.co/datasets/pixparse/pdfa-eng-wds/resolve/main/data/pdfa-eng-train-0001.tar\"\n",
    "web_dataset = wds.WebDataset(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3411da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bcd16b7d164ffead2e1504b6d25fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PDF data set byte format\n",
    "# taking sample from data set\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pixparse/pdfa-eng-wds\", streaming=True)\n",
    "small_dataset = dataset[\"train\"].take(5)\n",
    "\n",
    "\n",
    "# for doc in dataset[\"train\"]:\n",
    "#     print(len(doc[\"pdf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713f6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 1839ac7b8f395811156cc7d3e509f667\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 6368667\n",
      "c:\\Users\\adler\\Desktop\\rag_proj\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:docling.document_converter:Finished converting document 6368667 in 194.69 sec.\n",
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 1839ac7b8f395811156cc7d3e509f667\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 4102114\n",
      "INFO:docling.document_converter:Finished converting document 4102114 in 11.64 sec.\n",
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 1839ac7b8f395811156cc7d3e509f667\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 7912120\n",
      "INFO:docling.document_converter:Finished converting document 7912120 in 16.52 sec.\n",
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 1839ac7b8f395811156cc7d3e509f667\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 7292019\n",
      "c:\\Users\\adler\\Desktop\\rag_proj\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:docling.document_converter:Finished converting document 7292019 in 33.19 sec.\n",
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 1839ac7b8f395811156cc7d3e509f667\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 3180753\n",
      "c:\\Users\\adler\\Desktop\\rag_proj\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\adler\\Desktop\\rag_proj\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:docling.document_converter:Finished converting document 3180753 in 53.33 sec.\n"
     ]
    }
   ],
   "source": [
    "from src import *\n",
    "for doc in small_dataset:\n",
    "    pdf_doc = pdf_bytes_to_doc_stream(doc['pdf'], doc['__key__'])\n",
    "    extract_elements_from_pdf(pdf_doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19398d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6368667\n",
      "4102114\n",
      "7912120\n",
      "7292019\n",
      "3180753\n"
     ]
    }
   ],
   "source": [
    "for doc in small_dataset:\n",
    "    print(doc[\"__key__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_dataset.features\n",
    "\n",
    "\n",
    "items = list(small_dataset)\n",
    "last_item = items[-1]\n",
    "for page in last_item['json']['pages']:\n",
    "    print(type(page))\n",
    "    for item in page:\n",
    "        print(item)\n",
    "        print(type(item))\n",
    "\n",
    "# pdf_bytes = last_item[\"pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a001efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "def embed_on_ollama(text, model=\"bge-m3\", ollama_url=\"http://25.12.119.117:11435\", timeout=30):\n",
    "    print(text)\n",
    "    try:\n",
    "        # Prepare the request payload\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": str(text)\n",
    "        }\n",
    "        \n",
    "        print(f\"Sending request to {ollama_url}/api/embeddings...\")\n",
    "        print(payload)\n",
    "        # Make the API request with timeout\n",
    "        response = requests.post(\n",
    "            f\"{ollama_url}/api/embeddings\",\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(f\"Response status: {response.status_code}\")\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        #response.raise_for_status()\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        # Parse the response and extract the embedding\n",
    "        result = response.json()\n",
    "        print(\"results:\")\n",
    "        print(result)\n",
    "        embedding = result.get(\"embedding\")\n",
    "        \n",
    "        if len(embedding) == 0:\n",
    "            print(\"Error: No embedding found in response\")\n",
    "            print(f\"Response content: {result}\")\n",
    "            return None\n",
    "            \n",
    "        print(\"Received embedding successfully\")\n",
    "        print(embedding)\n",
    "        print(type(embedding))\n",
    "        return embedding\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out after {timeout} seconds\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Connection error - could not connect to {ollama_url}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        print(f\"Response text: {response.text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b506760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yee\n",
      "Sending request to http://25.12.119.117:11435/api/embeddings...\n",
      "{'model': 'bge-m3', 'prompt': 'yee'}\n",
      "Request timed out after 30 seconds\n"
     ]
    }
   ],
   "source": [
    "embed_on_ollama(\"yee\",model=\"bge-m3\", ollama_url=\"http://25.12.119.117:11435\", timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test if pdf is scanned\n",
    "\n",
    "def quick_scan_check(pdf_bytes):\n",
    "\n",
    "    import fitz\n",
    "    \n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    \n",
    "    # ◊ë◊ì◊ô◊ß◊™ ◊¢◊û◊ï◊ì ◊®◊ê◊©◊ï◊ü ◊ë◊ú◊ë◊ì (◊ú◊û◊î◊ô◊®◊ï◊™)\n",
    "    if len(doc) > 0:\n",
    "        page = doc[0]\n",
    "        text = page.get_text().strip()\n",
    "        images = page.get_images()\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # ◊ê◊ù ◊ê◊ô◊ü ◊ò◊ß◊°◊ò ◊ê◊ë◊ú ◊ô◊© ◊™◊û◊ï◊†◊ï◊™ - ◊õ◊†◊®◊ê◊î ◊°◊®◊ï◊ß\n",
    "        if len(text) < 10 and len(images) > 0:\n",
    "            return True, \"Likely scanned - minimal text, has images\"\n",
    "        elif len(text) > 100:\n",
    "            return False, \"Has substantial text - not scanned\"\n",
    "        else:\n",
    "            return None, \"Unclear - needs deeper analysis\"\n",
    "    \n",
    "    doc.close()\n",
    "    return None, \"Empty PDF\"\n",
    "\n",
    "is_scanned, reason = quick_scan_check(pdf_bytes)\n",
    "print(f\"Quick check - Is scanned: {is_scanned} ({reason})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = result.document\n",
    "print(result.document.name)\n",
    "for x in document:\n",
    "    print(x[0])\n",
    "# print(f'{list(result.input)}')\n",
    "# print(f'{list(result.model_fields_set)}')\n",
    "# print(f'{list(result.pages)}')\n",
    "# print(f'{list(result.status)}')\n",
    "# print(f'{list(result.timings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_document_content(document):\n",
    "    metadata = {\"name\": document.name,\n",
    "                \"version\": document.version,\n",
    "                \"origin\": document.origin,\n",
    "                \"number_of_pages\": len(document.pages),\n",
    "                \"text_count\": len(document.texts),\n",
    "                \"picture_count\": len(document.pictures),\n",
    "                \"table_count\": len(document.tables)\n",
    "                }\n",
    "    \n",
    "    if document.texts:\n",
    "        text_matadata = []\n",
    "        for text in document.texts:\n",
    "            metadata = {\"id\": text.self_ref,\n",
    "                        \"parent\": text.parent.cref,\n",
    "                        \"children\": [c.cref for c in text.children],\n",
    "                        \"orig\": text.orig,\n",
    "                        \"text\": text.text,\n",
    "                        \"formatting\": text.formatting,\n",
    "                        \"hyperlink\": text.hyperlink}\n",
    "            text_matadata.append(metadata)\n",
    "\n",
    "    if document.pictures:\n",
    "        pic_metadata = []\n",
    "        for pic in document.pictures:\n",
    "            metadata = {\"id\": pic.self_ref,\n",
    "                        \"parent\": pic.parent.cref,\n",
    "                        \"children\": [c.cref for c in pic.children],\n",
    "                        \"captions\":[c.cref for c in pic.captions],\n",
    "                        \"references\":pic.refereces,\n",
    "                        \"footnotes\": pic.footnotes,\n",
    "                        \"image\": pic.image,\n",
    "                        \"annotations\": pic.annotations}\n",
    "            pic_metadata.append(metadata)\n",
    "\n",
    "    \n",
    "\n",
    "    if document.tables:\n",
    "        table_metadata = []\n",
    "        for table in document.tables:\n",
    "            metadata = {\n",
    "                            \"id\": table.self_ref,\n",
    "                            \"parent\": table.parent.cref,\n",
    "                            \"children\": [c.cref for c in table.children],\n",
    "                            \"captions\":[c.cref for c in table.captions],\n",
    "                            \"references\":table.refereces,\n",
    "                            \"footnotes\": table.footnotes,\n",
    "                            \"image\": table.image,\n",
    "                            \"annotations\": table.annotations}\n",
    "            table_metadata.append(metadata)\n",
    "            \n",
    "    metadata = {\"name\": document.name,\n",
    "            \"version\": document.version,\n",
    "            \"origin\": document.origin,\n",
    "            \"number_of_pages\": len(document.pages),\n",
    "            \"text_count\": len(document.texts),\n",
    "            \"picture_count\": len(document.pictures),\n",
    "            \"table_count\": len(document.tables),\n",
    "            \"text_metadata\": text_matadata,\n",
    "            \"picture_metadata\": pic_metadata,\n",
    "            \"tables_metadata\": table_metadata\n",
    "            }\n",
    "    \n",
    "    # # ◊ñ◊ï◊í◊ï◊™ ◊û◊§◊™◊ó-◊¢◊®◊ö\n",
    "    # if document.key_value_items:\n",
    "    #     print(f\"üîë ◊ñ◊ï◊í◊ï◊™ ◊û◊§◊™◊ó-◊¢◊®◊ö ({len(document.key_value_items)} ◊§◊®◊ô◊ò◊ô◊ù):\")\n",
    "    #     print(\"-\" * 30)\n",
    "    #     for i, kv in enumerate(document.key_value_items, 1):\n",
    "    #         print(f\"  {i}. {kv}\")\n",
    "    #     print()\n",
    "\n",
    "#     # ◊ê◊ú◊û◊†◊ò◊ô ◊ò◊§◊°◊ô◊ù\n",
    "#     if document.form_items:\n",
    "#         print(f\"üìã ◊ê◊ú◊û◊†◊ò◊ô ◊ò◊§◊°◊ô◊ù ({len(document.form_items)} ◊§◊®◊ô◊ò◊ô◊ù):\")\n",
    "#         print(\"-\" * 30)\n",
    "#         for i, form in enumerate(document.form_items, 1):\n",
    "#             print(f\"  {i}. {form}\")\n",
    "#         print()\n",
    "    \n",
    "#     # ◊ß◊ë◊ï◊¶◊ï◊™\n",
    "#     if document.groups:\n",
    "#         print(f\"üë• ◊ß◊ë◊ï◊¶◊ï◊™ ({len(list(document.groups))} ◊§◊®◊ô◊ò◊ô◊ù):\")\n",
    "#         print(\"-\" * 30)\n",
    "#         for i, group in enumerate(document.groups, 1):\n",
    "#             print(f\"  {i}. {group}\")\n",
    "#         print()\n",
    "    \n",
    "    \n",
    "    # # ◊¢◊û◊ï◊ì◊ô◊ù\n",
    "    # if document.pages:\n",
    "    #     print(f\"üìÑ ◊¢◊û◊ï◊ì◊ô◊ù ({len(document.pages)} ◊§◊®◊ô◊ò◊ô◊ù):\")\n",
    "    #     print(\"-\" * 30)\n",
    "    #     for i, page in enumerate(document.pages):\n",
    "    #         print(f\"  {i}. {page}\")\n",
    "    #     print()\n",
    "#     print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# # ◊ì◊ï◊í◊û◊î ◊ú◊©◊ô◊û◊ï◊©:\n",
    "print_document_content(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "_log = logging.getLogger(__name__)\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_folder = Path(\"__file__\").parent / \"data\"\n",
    "input_doc_path = \"https://arxiv.org/pdf/2408.09869\"\n",
    "output_dir = Path(\"scratch\")\n",
    "\n",
    "# Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "# will destroy them for cleaning up memory.\n",
    "# This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n",
    "# scale=1 correspond of a standard 72 DPI image\n",
    "# The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n",
    "# with the image field\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "# pipeline_options.do_ocr = True\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "doc_filename = conv_res.input.file.stem\n",
    "\n",
    "# Save page images\n",
    "for page_no, page in conv_res.document.pages.items():\n",
    "    page_no = page.page_no\n",
    "    page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "    with page_image_filename.open(\"wb\") as fp:\n",
    "        page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "# Save images of figures and tables\n",
    "table_counter = 0\n",
    "picture_counter = 0\n",
    "for element, _level in conv_res.document.iterate_items():\n",
    "    if isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        element_image_filename = (\n",
    "            output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "        )\n",
    "        with element_image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    if isinstance(element, PictureItem):\n",
    "        picture_counter += 1\n",
    "        element_image_filename = (\n",
    "            output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "        )\n",
    "        with element_image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "# Save markdown with embedded pictures\n",
    "md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n",
    "conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "# Save markdown with externally referenced pictures\n",
    "md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# Save HTML with externally referenced pictures\n",
    "html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n",
    "conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c35564",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = conv_res.document\n",
    "print(\"=== ◊õ◊ú ◊î◊ò◊ß◊°◊ò◊ô◊ù ===\")\n",
    "for i, text_item in enumerate(document.texts):\n",
    "    print(f\"◊ò◊ß◊°◊ò {i}: {text_item.text}\")\n",
    "    print(text_item.)\n",
    "    print(f\"◊°◊ï◊í: {text_item.label}\")\n",
    "    if text_item.prov:  # ◊û◊ô◊ì◊¢ ◊¢◊ú ◊û◊ô◊ß◊ï◊ù\n",
    "        bbox = text_item.prov[0].bbox\n",
    "        print(f\"◊û◊ô◊ß◊ï◊ù: ◊¢◊û◊ï◊ì {text_item.prov[0].page_no}, (x: {bbox.l}, y: {bbox.t})\")\n",
    "    print(\"---\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_element_type(document):\n",
    "    \"\"\"◊†◊ô◊™◊ï◊ó ◊ú◊§◊ô ◊°◊ï◊í ◊ê◊ú◊û◊†◊ò\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ◊°◊§◊ô◊®◊™ ◊°◊ï◊í◊ô ◊ê◊ú◊û◊†◊ò◊ô◊ù\n",
    "    element_types = Counter([text.label for text in document.texts])\n",
    "    print(\"=== ◊°◊ï◊í◊ô ◊ê◊ú◊û◊†◊ò◊ô◊ù ===\")\n",
    "    for element_type, count in element_types.most_common():\n",
    "        print(f\"{element_type}: {count}\")\n",
    "    \n",
    "    # ◊ë◊ì◊ô◊ß◊î ◊û◊§◊ï◊®◊ò◊™ ◊ú◊§◊ô ◊°◊ï◊í\n",
    "    print(\"\\n=== ◊ì◊ï◊í◊û◊ê◊ï◊™ ◊ú◊§◊ô ◊°◊ï◊í ===\")\n",
    "    seen_types = set()\n",
    "    for text in document.texts:\n",
    "        if text.label not in seen_types:\n",
    "            seen_types.add(text.label)\n",
    "            print(f\"\\n{text.label}:\")\n",
    "            print(f\"  ◊ì◊ï◊í◊û◊î: {text.text[:100]}...\")\n",
    "            print(f\"  ◊™◊õ◊ï◊†◊ï◊™: {[attr for attr in dir(text) if not attr.startswith('_') and not callable(getattr(text, attr))]}\")\n",
    "            \n",
    "            # ◊ë◊ì◊ô◊ß◊î ◊û◊ô◊ï◊ó◊ì◊™ ◊ú◊°◊ï◊í◊ô◊ù ◊©◊ô◊õ◊ï◊ú◊ô◊ù ◊ú◊î◊õ◊ô◊ú ◊î◊§◊†◊ô◊ï◊™\n",
    "            if text.label in ['reference', 'footnote', 'caption']:\n",
    "                print(f\"  ‚≠ê ◊û◊¶◊ê◊™◊ô {text}!\")\n",
    "\n",
    "analyze_by_element_type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ab689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"◊ò◊ß◊°◊ò◊ô◊ù: {len(document.texts)}\")\n",
    "print(f\"◊ò◊ë◊ú◊ê◊ï◊™: {len(document.tables)}\")\n",
    "print(f\"◊™◊û◊ï◊†◊ï◊™: {len(document.pictures)}\")\n",
    "print(f\"◊ß◊ë◊ï◊¶◊ï◊™: {len(document.groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== ◊°◊î\\\"◊õ ◊™◊û◊ï◊†◊ï◊™: {len(document.pictures)} ===\")\n",
    "\n",
    "for i, picture in enumerate(document.pictures):\n",
    "    print(f\"\\n--- ◊™◊û◊ï◊†◊î {i+1} ---\")\n",
    "    print(f\"◊û◊ñ◊î◊î: {picture.self_ref}\")\n",
    "    print(f\"◊°◊ï◊í: {picture.label}\")\n",
    "    \n",
    "    # ◊õ◊ô◊™◊ï◊ë◊ô◊ù\n",
    "    if hasattr(picture, 'captions') and picture.captions:\n",
    "        print(f\"◊õ◊ô◊™◊ï◊ë◊ô◊ù ({len(picture.captions)}):\")\n",
    "        for j, caption in enumerate(picture.captions):\n",
    "            print(f\"  {j+1}. {caption}\")\n",
    "    else:\n",
    "        print(\"◊ê◊ô◊ü ◊õ◊ô◊™◊ï◊ë◊ô◊ù\")\n",
    "    \n",
    "    # ◊î◊§◊†◊ô◊ï◊™\n",
    "    if hasattr(picture, 'references') and picture.references:\n",
    "        print(f\"◊î◊§◊†◊ô◊ï◊™ ({len(picture.references)}):\")\n",
    "        for j, ref in enumerate(picture.references):\n",
    "            print(f\"  {j+1}. {ref.text}\")\n",
    "    else:\n",
    "        print(\"◊ê◊ô◊ü ◊î◊§◊†◊ô◊ï◊™\")\n",
    "    \n",
    "    # ◊û◊ô◊ß◊ï◊ù\n",
    "    if hasattr(picture, 'prov') and picture.prov:\n",
    "        prov = picture.prov[0]\n",
    "        print(f\"◊û◊ô◊ß◊ï◊ù: ◊¢◊û◊ï◊ì {prov.page_no}\")\n",
    "    \n",
    "    # ◊õ◊ú ◊î◊™◊õ◊ï◊†◊ï◊™ ◊î◊ñ◊û◊ô◊†◊ï◊™\n",
    "    attrs = [attr for attr in dir(picture) if not attr.startswith('_')]\n",
    "    print(f\"◊™◊õ◊ï◊†◊ï◊™ ◊ñ◊û◊ô◊†◊ï◊™: {attrs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a87eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ◊õ◊ú ◊î◊ò◊ë◊ú◊ê◊ï◊™ ===\")\n",
    "for i, table in enumerate(document.tables):\n",
    "    print(f\"◊ò◊ë◊ú◊î {i+1}:\")\n",
    "    # print(f\"◊™◊ï◊õ◊ü: {table}\")\n",
    "    \n",
    "    \n",
    "    # ◊í◊ô◊©◊î ◊ú◊û◊ë◊†◊î ◊î◊ò◊ë◊ú◊î (◊ê◊ù ◊ñ◊û◊ô◊ü)\n",
    "    if hasattr(table, 'data') and table.data:\n",
    "        print(f\"◊û◊°◊§◊® ◊©◊ï◊®◊ï◊™: {len(table.data.table_cells)}\")\n",
    "        \n",
    "    # ◊û◊ô◊ß◊ï◊ù ◊î◊ò◊ë◊ú◊î\n",
    "    if table.prov:\n",
    "        bbox = table.prov[0].bbox\n",
    "        print(f\"◊û◊ô◊ß◊ï◊ù: ◊¢◊û◊ï◊ì {table.prov[0].page_no}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ◊õ◊ú ◊î◊™◊û◊ï◊†◊ï◊™ ===\")\n",
    "for i, picture in enumerate(document.pictures):\n",
    "    print(f\"◊™◊û◊ï◊†◊î {i+1}:\")\n",
    "    print(f\"◊™◊ï◊ï◊ô◊™: {picture.label}\")\n",
    "    \n",
    "    # ◊õ◊ô◊™◊ï◊ë◊ô◊ù\n",
    "    if picture.captions:\n",
    "        for caption in picture.captions:\n",
    "            print(f\"◊õ◊ô◊™◊ï◊ë: {caption}\") #???\n",
    "    \n",
    "    # ◊û◊ô◊ß◊ï◊ù\n",
    "    if picture.prov:\n",
    "        bbox = picture.prov[0].bbox\n",
    "        print(f\"◊û◊ô◊ß◊ï◊ù: ◊¢◊û◊ï◊ì {picture.prov[0].page_no}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ◊ó◊ô◊§◊ï◊© ◊õ◊ï◊™◊®◊ï◊™\n",
    "headers = [text for text in document.texts if text.label in ['section_header', 'page_header']]\n",
    "print(f\"◊û◊¶◊ê◊™◊ô {len(headers)} ◊õ◊ï◊™◊®◊ï◊™\")\n",
    "\n",
    "# ◊ó◊ô◊§◊ï◊© ◊®◊©◊ô◊û◊ï◊™\n",
    "lists = [text for text in document.texts if text.label == 'list_item']\n",
    "print(f\"◊û◊¶◊ê◊™◊ô {len(lists)} ◊§◊®◊ô◊ò◊ô ◊®◊©◊ô◊û◊î\")\n",
    "\n",
    "# ◊ó◊ô◊§◊ï◊© ◊†◊ï◊°◊ó◊ê◊ï◊™\n",
    "formulas = [text for text in document.texts if text.label == 'formula']\n",
    "print(f\"◊û◊¶◊ê◊™◊ô {len(formulas)} ◊†◊ï◊°◊ó◊ê◊ï◊™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ◊ë◊ó◊ô◊†◊™ ◊û◊ò◊ê ◊ì◊ê◊ò◊î ◊©◊ú ◊ò◊ß◊°◊ò\n",
    "for i, text_item in enumerate(document.texts[:3]):  # 3 ◊î◊®◊ê◊©◊ï◊†◊ô◊ù\n",
    "    print(f\"=== ◊ò◊ß◊°◊ò {i+1} ===\")\n",
    "    print(f\"◊™◊ï◊õ◊ü: {text_item.text[:100]}...\")\n",
    "    print(f\"◊°◊ï◊í: {text_item.label}\")\n",
    "    print(f\"◊î◊ï◊®◊î: {text_item.parent}\")\n",
    "    print(f\"◊ô◊ú◊ì◊ô◊ù: {len(text_item.children)}\")\n",
    "    \n",
    "    # ◊û◊ô◊ì◊¢ ◊û◊ô◊ß◊ï◊ù\n",
    "    if text_item.prov:\n",
    "        prov = text_item.prov[0]\n",
    "        print(f\"◊¢◊û◊ï◊ì: {prov.page_no}\")\n",
    "        print(f\"◊û◊ô◊ß◊ï◊ù: x={prov.bbox.l:.1f}, y={prov.bbox.t:.1f}\")\n",
    "        print(f\"◊í◊ï◊ì◊ú: ◊®◊ï◊ó◊ë={prov.bbox.r-prov.bbox.l:.1f}, ◊í◊ï◊ë◊î={prov.bbox.b-prov.bbox.t:.1f}\")\n",
    "        print(f\"◊ò◊ï◊ï◊ó ◊™◊ï◊ï◊ô◊ù: {prov.charspan}\")\n",
    "    \n",
    "    # ◊û◊ô◊ì◊¢ ◊†◊ï◊°◊£\n",
    "    if text_item.:\n",
    "        print(f\"◊õ◊ô◊™◊ï◊ë◊ô◊ù: {len(text_item.captions)}\")\n",
    "    if text_item.references:\n",
    "        print(f\"◊î◊§◊†◊ô◊ï◊™: {len(text_item.references)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean cache ?\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "\n",
    "def find_all_cache_locations():\n",
    "    \"\"\"Find all possible Hugging Face cache locations\"\"\"\n",
    "    \n",
    "    print(\"=== HUGGING FACE CACHE LOCATIONS ===\\n\")\n",
    "    \n",
    "    # 1. Datasets cache\n",
    "    datasets_cache = datasets.config.HF_DATASETS_CACHE\n",
    "    print(f\"1. Datasets cache directory:\")\n",
    "    print(f\"   {datasets_cache}\")\n",
    "    print(f\"   Exists: {Path(datasets_cache).exists()}\")\n",
    "    if Path(datasets_cache).exists():\n",
    "        items = list(Path(datasets_cache).iterdir())\n",
    "        print(f\"   Items: {len(items)}\")\n",
    "        for item in items[:5]:  # Show first 5 items\n",
    "            print(f\"   - {item.name}\")\n",
    "        if len(items) > 5:\n",
    "            print(f\"   ... and {len(items) - 5} more\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Hub cache (for models and datasets)\n",
    "    hub_cache = huggingface_hub.constants.HF_HUB_CACHE\n",
    "    print(f\"2. Hub cache directory:\")\n",
    "    print(f\"   {hub_cache}\")\n",
    "    print(f\"   Exists: {Path(hub_cache).exists()}\")\n",
    "    if Path(hub_cache).exists():\n",
    "        items = list(Path(hub_cache).iterdir())\n",
    "        print(f\"   Items: {len(items)}\")\n",
    "        for item in items[:5]:\n",
    "            print(f\"   - {item.name}\")\n",
    "        if len(items) > 5:\n",
    "            print(f\"   ... and {len(items) - 5} more\")\n",
    "    print()\n",
    "    \n",
    "    # 3. Check environment variables\n",
    "    print(\"3. Environment variables:\")\n",
    "    env_vars = [\n",
    "        'HF_DATASETS_CACHE', 'HF_HUB_CACHE', 'HF_HOME', \n",
    "        'HUGGINGFACE_HUB_CACHE', 'TRANSFORMERS_CACHE'\n",
    "    ]\n",
    "    for var in env_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"   {var}: {value}\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Common cache locations to check\n",
    "    print(\"4. Other common cache locations:\")\n",
    "    common_locations = [\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path.home() / \".cache\" / \"huggingface\" / \"datasets\", \n",
    "        Path.home() / \".cache\" / \"huggingface\" / \"hub\",\n",
    "        Path(\"~/.huggingface\").expanduser(),\n",
    "        Path.cwd() / \".cache\",\n",
    "    ]\n",
    "    \n",
    "    for location in common_locations:\n",
    "        exists = location.exists()\n",
    "        print(f\"   {location}: {'EXISTS' if exists else 'Not found'}\")\n",
    "        if exists:\n",
    "            try:\n",
    "                items = list(location.iterdir())\n",
    "                print(f\"     Items: {len(items)}\")\n",
    "                # Look for dataset-specific folders\n",
    "                for item in items:\n",
    "                    if 'd4rk3r' in item.name.lower() or 'resumes' in item.name.lower():\n",
    "                        print(f\"     FOUND DATASET CACHE: {item}\")\n",
    "            except PermissionError:\n",
    "                print(f\"     (Permission denied)\")\n",
    "    print()\n",
    "\n",
    "# Run the function\n",
    "find_all_cache_locations()\n",
    "\n",
    "# Also check the current working directory for any hidden cache\n",
    "print(\"5. Checking current working directory for cache:\")\n",
    "cwd = Path.cwd()\n",
    "for item in cwd.iterdir():\n",
    "    if item.is_dir() and ('cache' in item.name.lower() or item.name.startswith('.')):\n",
    "        print(f\"   Found: {item}\")\n",
    "        if 'huggingface' in item.name.lower() or 'datasets' in item.name.lower():\n",
    "            print(f\"   *** This might be your cache! ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean cahce\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "\n",
    "# Clear datasets cache\n",
    "datasets_cache = Path(datasets.config.HF_DATASETS_CACHE)\n",
    "if datasets_cache.exists():\n",
    "    print(f\"Removing datasets cache: {datasets_cache}\")\n",
    "    shutil.rmtree(datasets_cache)\n",
    "\n",
    "# Clear hub cache  \n",
    "hub_cache = Path(huggingface_hub.constants.HF_HUB_CACHE)\n",
    "if hub_cache.exists():\n",
    "    print(f\"Removing hub cache: {hub_cache}\")\n",
    "    shutil.rmtree(hub_cache)\n",
    "\n",
    "# Clear common cache locations\n",
    "common_caches = [\n",
    "    Path.home() / \".cache\" / \"huggingface\",\n",
    "    Path(\"~/.huggingface\").expanduser(),\n",
    "]\n",
    "\n",
    "for cache_path in common_caches:\n",
    "    if cache_path.exists():\n",
    "        print(f\"Removing: {cache_path}\")\n",
    "        shutil.rmtree(cache_path)\n",
    "\n",
    "print(\"All caches cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c488a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
